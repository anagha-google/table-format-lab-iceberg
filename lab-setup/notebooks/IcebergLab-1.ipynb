{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4529a790-8425-4600-841a-4b094b82eaa8",
   "metadata": {},
   "source": [
    "# Apache Iceberg Lab \n",
    "## Unit 1: Create a base Parquet table\n",
    "Create a base table in Parquet, off of the Kaggle Lending Club Loan dataset, preloaded into your GCS data bucket in directory parquet-source."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a31c4fd-d465-4f52-8e56-3775bf499abc",
   "metadata": {},
   "source": [
    "### 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1321bce9-178c-4065-8187-0a5728c1a370",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import SparkSession\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed295a74-ed1d-4b5d-831a-1b5dcf73c36f",
   "metadata": {},
   "source": [
    "### 2. Create a Spark session powered by Cloud Dataproc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b383d5ab-a0b9-45ab-a232-34d88f2a0065",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://gdpic-srvls-session-7b192709-4525-4de5-8984-02c3c6557b73-m.c.nikhim-iceberg-lab.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://gdpic-srvls-session-7b192709-4525-4de5-8984-02c3c6557b73-m:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f98e0978850>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('Loan Analysis').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cd13e6-f3f5-4f2c-b4fc-d7e2660c6206",
   "metadata": {},
   "source": [
    "### 3. Declare variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5596e31b-749a-4702-8879-6f05f9ff0c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ID:  nikhim-iceberg-lab\n"
     ]
    }
   ],
   "source": [
    "project_id_output = !gcloud config list --format \"value(core.project)\" 2>/dev/null\n",
    "PROJECT_ID = project_id_output[0]\n",
    "print(\"PROJECT_ID: \", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "471c6743-a058-462b-851a-f34323f36243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_NAME:  nikhim-iceberg-lab\n"
     ]
    }
   ],
   "source": [
    "project_name_output = !gcloud projects describe $PROJECT_ID | grep name | cut -d':' -f2 | xargs\n",
    "PROJECT_NAME = project_name_output[0]\n",
    "print(\"PROJECT_NAME: \", PROJECT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61929dc6-a083-433c-8a13-3d39d9c4a4a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_NUMBER:  928505941962\n"
     ]
    }
   ],
   "source": [
    "project_number_output = !gcloud projects describe $PROJECT_ID | grep projectNumber | cut -d':' -f2 | xargs\n",
    "PROJECT_NUMBER = project_number_output[0]\n",
    "print(\"PROJECT_NUMBER: \", PROJECT_NUMBER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8b79fd2-5243-41a4-ae87-e7f9dd87cf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_LAKE_ROOT_PATH= f\"gs://iceberg-data-bucket-{PROJECT_NUMBER}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b0ec69d-c609-4fc5-9bdf-80025defb235",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_SOURCE_FQ_GCS_PATH = f\"{DATA_LAKE_ROOT_PATH}/parquet-source/*\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8ef5b6-3db7-42da-bcb8-ef7b5efece68",
   "metadata": {},
   "source": [
    "### 4. Explore the raw loans data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59768a80-65cd-47ab-8870-f9e7c82b2811",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://iceberg-data-bucket-928505941962/parquet-source/\n",
      "gs://iceberg-data-bucket-928505941962/parquet-source/loans_raw_1.snappy.parquet\n",
      "gs://iceberg-data-bucket-928505941962/parquet-source/loans_raw_2.snappy.parquet\n",
      "gs://iceberg-data-bucket-928505941962/parquet-source/loans_raw_3.snappy.parquet\n",
      "gs://iceberg-data-bucket-928505941962/parquet-source/loans_raw_4.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls -r $RAW_SOURCE_FQ_GCS_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f28c3a11-793a-4a31-a00a-1582c0c04431",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rawDF = spark.read.parquet(RAW_SOURCE_FQ_GCS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c35520d8-4d9d-4d72-b033-306c742d892f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- member_id: string (nullable = true)\n",
      " |-- loan_amnt: float (nullable = true)\n",
      " |-- funded_amnt: integer (nullable = true)\n",
      " |-- funded_amnt_inv: double (nullable = true)\n",
      " |-- term: string (nullable = true)\n",
      " |-- int_rate: string (nullable = true)\n",
      " |-- installment: double (nullable = true)\n",
      " |-- grade: string (nullable = true)\n",
      " |-- sub_grade: string (nullable = true)\n",
      " |-- emp_title: string (nullable = true)\n",
      " |-- emp_length: string (nullable = true)\n",
      " |-- home_ownership: string (nullable = true)\n",
      " |-- annual_inc: float (nullable = true)\n",
      " |-- verification_status: string (nullable = true)\n",
      " |-- loan_status: string (nullable = true)\n",
      " |-- pymnt_plan: string (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      " |-- desc: string (nullable = true)\n",
      " |-- purpose: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- zip_code: string (nullable = true)\n",
      " |-- addr_state: string (nullable = true)\n",
      " |-- dti: float (nullable = true)\n",
      " |-- delinq_2yrs: float (nullable = true)\n",
      " |-- earliest_cr_line: string (nullable = true)\n",
      " |-- inq_last_6mths: string (nullable = true)\n",
      " |-- mths_since_last_delinq: integer (nullable = true)\n",
      " |-- mths_since_last_record: string (nullable = true)\n",
      " |-- open_acc: integer (nullable = true)\n",
      " |-- pub_rec: integer (nullable = true)\n",
      " |-- revol_bal: integer (nullable = true)\n",
      " |-- revol_util: string (nullable = true)\n",
      " |-- total_acc: float (nullable = true)\n",
      " |-- initial_list_status: string (nullable = true)\n",
      " |-- out_prncp: string (nullable = true)\n",
      " |-- out_prncp_inv: double (nullable = true)\n",
      " |-- total_pymnt: string (nullable = true)\n",
      " |-- total_pymnt_inv: double (nullable = true)\n",
      " |-- total_rec_prncp: double (nullable = true)\n",
      " |-- total_rec_int: double (nullable = true)\n",
      " |-- total_rec_late_fee: double (nullable = true)\n",
      " |-- recoveries: double (nullable = true)\n",
      " |-- collection_recovery_fee: double (nullable = true)\n",
      " |-- last_pymnt_d: string (nullable = true)\n",
      " |-- last_pymnt_amnt: string (nullable = true)\n",
      " |-- next_pymnt_d: string (nullable = true)\n",
      " |-- last_credit_pull_d: string (nullable = true)\n",
      " |-- collections_12_mths_ex_med: string (nullable = true)\n",
      " |-- mths_since_last_major_derog: string (nullable = true)\n",
      " |-- policy_code: string (nullable = true)\n",
      " |-- application_type: string (nullable = true)\n",
      " |-- annual_inc_joint: string (nullable = true)\n",
      " |-- dti_joint: double (nullable = true)\n",
      " |-- verification_status_joint: string (nullable = true)\n",
      " |-- acc_now_delinq: integer (nullable = true)\n",
      " |-- tot_coll_amt: integer (nullable = true)\n",
      " |-- tot_cur_bal: integer (nullable = true)\n",
      " |-- open_acc_6m: integer (nullable = true)\n",
      " |-- open_il_6m: integer (nullable = true)\n",
      " |-- open_il_12m: integer (nullable = true)\n",
      " |-- open_il_24m: integer (nullable = true)\n",
      " |-- mths_since_rcnt_il: integer (nullable = true)\n",
      " |-- total_bal_il: integer (nullable = true)\n",
      " |-- il_util: integer (nullable = true)\n",
      " |-- open_rv_12m: integer (nullable = true)\n",
      " |-- open_rv_24m: integer (nullable = true)\n",
      " |-- max_bal_bc: integer (nullable = true)\n",
      " |-- all_util: integer (nullable = true)\n",
      " |-- total_rev_hi_lim: integer (nullable = true)\n",
      " |-- inq_fi: integer (nullable = true)\n",
      " |-- total_cu_tl: integer (nullable = true)\n",
      " |-- inq_last_12m: integer (nullable = true)\n",
      " |-- acc_open_past_24mths: integer (nullable = true)\n",
      " |-- avg_cur_bal: integer (nullable = true)\n",
      " |-- bc_open_to_buy: integer (nullable = true)\n",
      " |-- bc_util: double (nullable = true)\n",
      " |-- chargeoff_within_12_mths: double (nullable = true)\n",
      " |-- delinq_amnt: integer (nullable = true)\n",
      " |-- mo_sin_old_il_acct: double (nullable = true)\n",
      " |-- mo_sin_old_rev_tl_op: integer (nullable = true)\n",
      " |-- mo_sin_rcnt_rev_tl_op: integer (nullable = true)\n",
      " |-- mo_sin_rcnt_tl: integer (nullable = true)\n",
      " |-- mort_acc: integer (nullable = true)\n",
      " |-- mths_since_recent_bc: integer (nullable = true)\n",
      " |-- mths_since_recent_bc_dlq: integer (nullable = true)\n",
      " |-- mths_since_recent_inq: integer (nullable = true)\n",
      " |-- mths_since_recent_revol_delinq: integer (nullable = true)\n",
      " |-- num_accts_ever_120_pd: integer (nullable = true)\n",
      " |-- num_actv_bc_tl: integer (nullable = true)\n",
      " |-- num_actv_rev_tl: integer (nullable = true)\n",
      " |-- num_bc_sats: integer (nullable = true)\n",
      " |-- num_bc_tl: integer (nullable = true)\n",
      " |-- num_il_tl: integer (nullable = true)\n",
      " |-- num_op_rev_tl: integer (nullable = true)\n",
      " |-- num_rev_accts: integer (nullable = true)\n",
      " |-- num_rev_tl_bal_gt_0: integer (nullable = true)\n",
      " |-- num_sats: integer (nullable = true)\n",
      " |-- num_tl_120dpd_2m: integer (nullable = true)\n",
      " |-- num_tl_30dpd: integer (nullable = true)\n",
      " |-- num_tl_90g_dpd_24m: integer (nullable = true)\n",
      " |-- num_tl_op_past_12m: integer (nullable = true)\n",
      " |-- pct_tl_nvr_dlq: double (nullable = true)\n",
      " |-- percent_bc_gt_75: double (nullable = true)\n",
      " |-- pub_rec_bankruptcies: integer (nullable = true)\n",
      " |-- tax_liens: integer (nullable = true)\n",
      " |-- tot_hi_cred_lim: integer (nullable = true)\n",
      " |-- total_bal_ex_mort: integer (nullable = true)\n",
      " |-- total_bc_limit: integer (nullable = true)\n",
      " |-- total_il_high_credit_limit: integer (nullable = true)\n",
      " |-- revol_bal_joint: integer (nullable = true)\n",
      " |-- sec_app_earliest_cr_line: string (nullable = true)\n",
      " |-- sec_app_inq_last_6mths: integer (nullable = true)\n",
      " |-- sec_app_mort_acc: integer (nullable = true)\n",
      " |-- sec_app_open_acc: integer (nullable = true)\n",
      " |-- sec_app_revol_util: double (nullable = true)\n",
      " |-- sec_app_open_il_6m: integer (nullable = true)\n",
      " |-- sec_app_num_rev_accts: integer (nullable = true)\n",
      " |-- sec_app_chargeoff_within_12_mths: integer (nullable = true)\n",
      " |-- sec_app_collections_12_mths_ex_med: integer (nullable = true)\n",
      " |-- sec_app_mths_since_last_major_derog: integer (nullable = true)\n",
      " |-- hardship_flag: string (nullable = true)\n",
      " |-- hardship_type: string (nullable = true)\n",
      " |-- hardship_reason: string (nullable = true)\n",
      " |-- hardship_status: string (nullable = true)\n",
      " |-- deferral_term: integer (nullable = true)\n",
      " |-- hardship_amount: double (nullable = true)\n",
      " |-- hardship_start_date: string (nullable = true)\n",
      " |-- hardship_end_date: string (nullable = true)\n",
      " |-- payment_plan_start_date: string (nullable = true)\n",
      " |-- hardship_length: integer (nullable = true)\n",
      " |-- hardship_dpd: integer (nullable = true)\n",
      " |-- hardship_loan_status: string (nullable = true)\n",
      " |-- orig_projected_additional_accrued_interest: double (nullable = true)\n",
      " |-- hardship_payoff_balance_amount: double (nullable = true)\n",
      " |-- hardship_last_payment_amount: double (nullable = true)\n",
      " |-- issue_d: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rawDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e870e4b9-0c83-49af-9590-074dfd503ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/02/10 14:55:33 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "rawDF=rawDF.na.drop(subset=[\"addr_state\"])\n",
    "rawDF.createOrReplaceTempView(\"loans_raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "648aec0d-f1a2-461a-beff-8d815ac84e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:===================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+----------+\n",
      "|state|       loan_status|loan_count|\n",
      "+-----+------------------+----------+\n",
      "|   CA|        Fully Paid|     24349|\n",
      "|   CO|           Current|      4541|\n",
      "|   CO|        Fully Paid|      3900|\n",
      "|   NC|           Current|      6529|\n",
      "|   KY| Late (16-30 days)|        12|\n",
      "|   NC| Late (16-30 days)|        60|\n",
      "|   PA|Late (31-120 days)|       215|\n",
      "|   IN|        Fully Paid|      2468|\n",
      "|   ME|   In Grace Period|        12|\n",
      "|   WY|        Fully Paid|       353|\n",
      "|   CO|   In Grace Period|        70|\n",
      "|   ND|        Fully Paid|       100|\n",
      "|   DE|Late (31-120 days)|        18|\n",
      "|   TX|        Fully Paid|     12662|\n",
      "|   DC|        Fully Paid|       451|\n",
      "|   MN|       Charged Off|       763|\n",
      "|   IL|   In Grace Period|       159|\n",
      "|   OK|   In Grace Period|        29|\n",
      "|   TX|   In Grace Period|       316|\n",
      "|   VT|       Charged Off|        47|\n",
      "+-----+------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Count total loans\n",
    "spark.sql(\"select addr_state as state,loan_status, count(*) as loan_count from loans_raw group by addr_state,loan_status\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a1b86b4-593c-4736-9394-9d8a38c89311",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:============================================>              (6 + 2) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+\n",
      "|count(DISTINCT addr_state)|\n",
      "+--------------------------+\n",
      "|52                        |\n",
      "+--------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# How many distinct states?\n",
    "spark.sql(\"select count(distinct addr_state) from loans_raw\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2617874f-eedc-4714-a8ce-03a17e943fbe",
   "metadata": {},
   "source": [
    "### 5. Cleanse the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7a3a70a1-1d3b-4931-b6f5-42ba6dd51301",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(addr_state='AZ'),\n",
       " Row(addr_state='SC'),\n",
       " Row(addr_state='LA'),\n",
       " Row(addr_state='MN'),\n",
       " Row(addr_state='NJ'),\n",
       " Row(addr_state='DC'),\n",
       " Row(addr_state='OR'),\n",
       " Row(addr_state='VA'),\n",
       " Row(addr_state='RI'),\n",
       " Row(addr_state='KY'),\n",
       " Row(addr_state='WY'),\n",
       " Row(addr_state='NH'),\n",
       " Row(addr_state='MI'),\n",
       " Row(addr_state='NV'),\n",
       " Row(addr_state='WI'),\n",
       " Row(addr_state='ID'),\n",
       " Row(addr_state='CA'),\n",
       " Row(addr_state='CT'),\n",
       " Row(addr_state='NE'),\n",
       " Row(addr_state='MT'),\n",
       " Row(addr_state='NC'),\n",
       " Row(addr_state='VT'),\n",
       " Row(addr_state='MD'),\n",
       " Row(addr_state='DE'),\n",
       " Row(addr_state='MO'),\n",
       " Row(addr_state='IL'),\n",
       " Row(addr_state='ME'),\n",
       " Row(addr_state='WA'),\n",
       " Row(addr_state='ND'),\n",
       " Row(addr_state='MS'),\n",
       " Row(addr_state='AL'),\n",
       " Row(addr_state='IN'),\n",
       " Row(addr_state='OH'),\n",
       " Row(addr_state='TN'),\n",
       " Row(addr_state='NM'),\n",
       " Row(addr_state='IA'),\n",
       " Row(addr_state='PA'),\n",
       " Row(addr_state='SD'),\n",
       " Row(addr_state='NY'),\n",
       " Row(addr_state='TX'),\n",
       " Row(addr_state='WV'),\n",
       " Row(addr_state='GA'),\n",
       " Row(addr_state='MA'),\n",
       " Row(addr_state='KS'),\n",
       " Row(addr_state='FL'),\n",
       " Row(addr_state='CO'),\n",
       " Row(addr_state='AK'),\n",
       " Row(addr_state='AR'),\n",
       " Row(addr_state='OK'),\n",
       " Row(addr_state='UT'),\n",
       " Row(addr_state='HI'),\n",
       " Row(addr_state='debt_consolidation')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Distinct states\n",
    "spark.sql(\"select distinct addr_state from loans_raw\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e587ccd3-bde0-46e8-936d-c8ed13ac3b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove data with invalid states\n",
    "cleansedSubsetDF=spark.sql(\"select * from loans_raw where addr_state not in ('debt_consolidation')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "655c3269-0d51-48f1-8dfd-f8e6d22f8def",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleansed subset row count=444031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:====================================>                     (5 + 3) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleansed subset distinct state count=51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Quick counts\n",
    "count1=cleansedSubsetDF.count()\n",
    "print(f\"Cleansed subset row count={count1}\")\n",
    "\n",
    "count2=cleansedSubsetDF.select(\"addr_state\").distinct().count()\n",
    "print(f\"Cleansed subset distinct state count={count2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174df43b-9978-447e-8529-cc8e43cfa05d",
   "metadata": {},
   "source": [
    "### 6. Persist the cleansed data to the data lake, as Parquet & create an external table definition on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b63ce4c6-d663-4750-8b02-87f0db676956",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Persist the cleaned data\n",
    "cleansedSubsetDF.coalesce(3).write.format(\"parquet\").mode(\"overwrite\").save(f\"{DATA_LAKE_ROOT_PATH}/parquet-cleansed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9dc7ff40-23e4-4e6b-b3c8-bfb91f9e7e7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'thrift://10.93.64.15:9080'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if we are using the Dataproc Metastore\n",
    "spark.sparkContext._conf.get(\"spark.hive.metastore.uris\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1ec38945-9a15-4227-8858-1cd413592439",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/etc/spark/conf/ivysettings.xml will be used\n",
      "23/02/10 15:01:40 INFO DependencyResolver: ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/etc/spark/conf/ivysettings.xml will be used\n",
      "23/02/10 15:01:41 INFO metastore: Trying to connect to metastore with URI thrift://10.93.64.15:9080\n",
      "23/02/10 15:01:41 INFO metastore: Opened a connection to metastore, current connections: 1\n",
      "23/02/10 15:01:41 INFO metastore: Connected to metastore.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|default  |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a database if it does not exist already\n",
    "spark.sql(\"SHOW DATABASES;\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2f6b1726-2e4a-4a23-9901-d77dd21d5d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a database if it does not exist already\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS loan_db;\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1c7cb0bb-ff4b-4772-ba5e-5ac537a0a1cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "|  loan_db|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2f547356-b878-4166-9ac1-523b570b6ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/02/10 15:01:58 INFO metastore: Trying to connect to metastore with URI thrift://10.93.64.15:9080\n",
      "23/02/10 15:01:58 INFO metastore: Opened a connection to metastore, current connections: 2\n",
      "23/02/10 15:01:58 INFO metastore: Connected to metastore.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/02/10 15:01:59 INFO SQLStdHiveAccessController: Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=96fbc719-6cd0-4361-89b8-0180ce31ef04, clientType=HIVECLI]\n",
      "23/02/10 15:01:59 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "23/02/10 15:01:59 INFO metastore: Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook\n",
      "23/02/10 15:01:59 INFO metastore: Closed a connection to metastore, current connections: 1\n",
      "23/02/10 15:01:59 INFO metastore: Trying to connect to metastore with URI thrift://10.93.64.15:9080\n",
      "23/02/10 15:01:59 INFO metastore: Opened a connection to metastore, current connections: 2\n",
      "23/02/10 15:01:59 INFO metastore: Connected to metastore.\n",
      "23/02/10 15:01:59 INFO metastore: Trying to connect to metastore with URI thrift://10.93.64.15:9080\n",
      "23/02/10 15:01:59 INFO metastore: Opened a connection to metastore, current connections: 3\n",
      "23/02/10 15:01:59 INFO metastore: Connected to metastore.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create an external table defintion on the parquet files\n",
    "spark.sql(\"DROP TABLE IF EXISTS loan_db.loans_cleansed_parquet;\").show(truncate=False)\n",
    "spark.sql(f\"CREATE TABLE loan_db.loans_cleansed_parquet USING parquet LOCATION '{DATA_LAKE_ROOT_PATH}/parquet-cleansed';\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9104153c-d55f-4e5f-a4ec-37ea1919806b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://iceberg-data-bucket-928505941962/parquet-cleansed/:\n",
      "gs://iceberg-data-bucket-928505941962/parquet-cleansed/\n",
      "gs://iceberg-data-bucket-928505941962/parquet-cleansed/_SUCCESS\n",
      "gs://iceberg-data-bucket-928505941962/parquet-cleansed/part-00000-90b80b5a-8355-4441-b654-b72820816fea-c000.snappy.parquet\n",
      "gs://iceberg-data-bucket-928505941962/parquet-cleansed/part-00001-90b80b5a-8355-4441-b654-b72820816fea-c000.snappy.parquet\n",
      "gs://iceberg-data-bucket-928505941962/parquet-cleansed/part-00002-90b80b5a-8355-4441-b654-b72820816fea-c000.snappy.parquet\n",
      "\n",
      "gs://iceberg-data-bucket-928505941962/parquet-source/:\n",
      "gs://iceberg-data-bucket-928505941962/parquet-source/\n",
      "gs://iceberg-data-bucket-928505941962/parquet-source/loans_raw_1.snappy.parquet\n",
      "gs://iceberg-data-bucket-928505941962/parquet-source/loans_raw_2.snappy.parquet\n",
      "gs://iceberg-data-bucket-928505941962/parquet-source/loans_raw_3.snappy.parquet\n",
      "gs://iceberg-data-bucket-928505941962/parquet-source/loans_raw_4.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "# Review what's in the data lake\n",
    "!gsutil ls -r $DATA_LAKE_ROOT_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881f4f3e-a4df-46a1-b032-bf97bb209afb",
   "metadata": {},
   "source": [
    "### 7. Create a parquet table on the base parquet dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "717e4782-0936-4f66-b60a-6404403ae7a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CommandException: 1 files/objects could not be removed.\n"
     ]
    }
   ],
   "source": [
    "# Remove any residual files from potential prior run\n",
    "!gsutil rm -rf $DATA_LAKE_ROOT_PATH/parquet-consumable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "813f55f8-cff6-4e7a-9305-0eb8ff4803c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create table in Parquet off of the cleansed raw data\n",
    "spark.sql(\"DROP TABLE IF EXISTS loan_db.loans_by_state_parquet;\").show(truncate=False)\n",
    "spark.sql(f\"CREATE TABLE loan_db.loans_by_state_parquet USING parquet LOCATION '{DATA_LAKE_ROOT_PATH}/parquet-consumable' AS SELECT addr_state, count(loan_status) as loan_count FROM loan_db.loans_cleansed_parquet GROUP BY addr_state;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e5c604b1-3ded-420d-8bf5-0c35e2c61aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------------------+-----------+\n",
      "|namespace|tableName             |isTemporary|\n",
      "+---------+----------------------+-----------+\n",
      "|loan_db  |loans_by_state_parquet|false      |\n",
      "|loan_db  |loans_cleansed_parquet|false      |\n",
      "|         |loans_raw             |true       |\n",
      "+---------+----------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the Dataproc metastore for the new table\n",
    "spark.sql(\"show tables from loan_db;\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "abdeedf7-cc40-47a4-a04e-1d751f114dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|addr_state|loan_count|\n",
      "+----------+----------+\n",
      "|AZ        |10318     |\n",
      "|SC        |5460      |\n",
      "|LA        |5284      |\n",
      "|MN        |8031      |\n",
      "|NJ        |16367     |\n",
      "|DC        |1059      |\n",
      "|OR        |5258      |\n",
      "|VA        |12775     |\n",
      "|RI        |1968      |\n",
      "|KY        |4287      |\n",
      "|WY        |964       |\n",
      "|NH        |2148      |\n",
      "|MI        |11638     |\n",
      "|NV        |6309      |\n",
      "|WI        |5798      |\n",
      "|ID        |522       |\n",
      "|CA        |62090     |\n",
      "|CT        |6767      |\n",
      "|NE        |1299      |\n",
      "|MT        |1220      |\n",
      "+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# List some data\n",
    "spark.sql(\"select * from loan_db.loans_by_state_parquet\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cedac28-84b0-46e9-be98-ac78d8b75afc",
   "metadata": {},
   "source": [
    "### 8. Review what is in the data lake"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb00e7a-786e-42b5-b3be-f1fee47e883a",
   "metadata": {},
   "source": [
    "Review cell #8. There was just one directory - parquet-source. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cb3ae7-119e-4e06-acb2-92eaba37f739",
   "metadata": {},
   "source": [
    "Next review cell #19. A directory called parquet-cleansed was added. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b955539-15af-4209-9455-0a73ab096d5f",
   "metadata": {},
   "source": [
    "At the end of this notebook, we also have a parquet-consumable directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ad20e372-f574-4092-a535-0333432d457d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://iceberg-data-bucket-928505941962/parquet-cleansed/:\n",
      "gs://iceberg-data-bucket-928505941962/parquet-cleansed/\n",
      "gs://iceberg-data-bucket-928505941962/parquet-cleansed/_SUCCESS\n",
      "gs://iceberg-data-bucket-928505941962/parquet-cleansed/part-00000-90b80b5a-8355-4441-b654-b72820816fea-c000.snappy.parquet\n",
      "gs://iceberg-data-bucket-928505941962/parquet-cleansed/part-00001-90b80b5a-8355-4441-b654-b72820816fea-c000.snappy.parquet\n",
      "gs://iceberg-data-bucket-928505941962/parquet-cleansed/part-00002-90b80b5a-8355-4441-b654-b72820816fea-c000.snappy.parquet\n",
      "\n",
      "gs://iceberg-data-bucket-928505941962/parquet-consumable/:\n",
      "gs://iceberg-data-bucket-928505941962/parquet-consumable/\n",
      "gs://iceberg-data-bucket-928505941962/parquet-consumable/_SUCCESS\n",
      "gs://iceberg-data-bucket-928505941962/parquet-consumable/part-00000-4d27f2ac-f9cc-4c69-820b-666c9050e00b-c000.snappy.parquet\n",
      "\n",
      "gs://iceberg-data-bucket-928505941962/parquet-source/:\n",
      "gs://iceberg-data-bucket-928505941962/parquet-source/\n",
      "gs://iceberg-data-bucket-928505941962/parquet-source/loans_raw_1.snappy.parquet\n",
      "gs://iceberg-data-bucket-928505941962/parquet-source/loans_raw_2.snappy.parquet\n",
      "gs://iceberg-data-bucket-928505941962/parquet-source/loans_raw_3.snappy.parquet\n",
      "gs://iceberg-data-bucket-928505941962/parquet-source/loans_raw_4.snappy.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/02/10 15:04:43 WARN JavaUtils: Attempt to delete using native Unix OS command failed for path = /var/tmp/spark/local-dir/blockmgr-fd92277c-63e0-4a2b-bfe3-c1863c34a324. Falling back to Java IO way\n",
      "java.io.IOException: Failed to delete: /var/tmp/spark/local-dir/blockmgr-fd92277c-63e0-4a2b-bfe3-c1863c34a324\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingUnixNative(JavaUtils.java:171)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:110)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:91)\n",
      "\tat org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1193)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1(DiskBlockManager.scala:318)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1$adapted(DiskBlockManager.scala:314)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.doStop(DiskBlockManager.scala:314)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$addShutdownHook$2(DiskBlockManager.scala:296)\n",
      "\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2048)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/02/10 15:04:43 WARN JavaUtils: Attempt to delete using native Unix OS command failed for path = /var/tmp/spark/local-dir/blockmgr-fd92277c-63e0-4a2b-bfe3-c1863c34a324/22. Falling back to Java IO way\n",
      "java.io.IOException: Failed to delete: /var/tmp/spark/local-dir/blockmgr-fd92277c-63e0-4a2b-bfe3-c1863c34a324/22\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingUnixNative(JavaUtils.java:171)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:110)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:128)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:91)\n",
      "\tat org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1193)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1(DiskBlockManager.scala:318)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1$adapted(DiskBlockManager.scala:314)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.doStop(DiskBlockManager.scala:314)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$addShutdownHook$2(DiskBlockManager.scala:296)\n",
      "\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2048)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/02/10 15:04:44 ERROR Utils: Uncaught exception in thread Thread-3\n",
      "java.lang.NullPointerException\n",
      "\tat com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.getGcsFs(GoogleHadoopFileSystemBase.java:1277)\n",
      "\tat com.google.cloud.hadoop.fs.gcs.GoogleHadoopSyncableOutputStream.commitCurrentFile(GoogleHadoopSyncableOutputStream.java:335)\n",
      "\tat com.google.cloud.hadoop.fs.gcs.GoogleHadoopSyncableOutputStream.close(GoogleHadoopSyncableOutputStream.java:214)\n",
      "\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77)\n",
      "\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n",
      "\tat com.github.luben.zstd.ZstdOutputStreamNoFinalizer.close(ZstdOutputStreamNoFinalizer.java:283)\n",
      "\tat com.github.luben.zstd.ZstdOutputStreamNoFinalizer.close(ZstdOutputStreamNoFinalizer.java:258)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implClose(StreamEncoder.java:341)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.close(StreamEncoder.java:161)\n",
      "\tat java.base/java.io.OutputStreamWriter.close(OutputStreamWriter.java:255)\n",
      "\tat java.base/java.io.PrintWriter.close(PrintWriter.java:415)\n",
      "\tat org.apache.spark.deploy.history.EventLogFileWriter.$anonfun$closeWriter$1(EventLogFileWriters.scala:127)\n",
      "\tat org.apache.spark.deploy.history.EventLogFileWriter.$anonfun$closeWriter$1$adapted(EventLogFileWriters.scala:127)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.deploy.history.EventLogFileWriter.closeWriter(EventLogFileWriters.scala:127)\n",
      "\tat org.apache.spark.deploy.history.SingleEventLogFileWriter.stop(EventLogFileWriters.scala:237)\n",
      "\tat org.apache.spark.scheduler.EventLoggingListener.stop(EventLoggingListener.scala:283)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$stop$17(SparkContext.scala:2109)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$stop$17$adapted(SparkContext.scala:2109)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$stop$16(SparkContext.scala:2109)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1471)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2109)\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.stop(JavaSparkContext.scala:550)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls -r $DATA_LAKE_ROOT_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f10e97-18fe-4396-bb0a-78de19163953",
   "metadata": {},
   "source": [
    "We will use the data under the parquet-consumable directory in the next unit, and create Apache Iceberg table off of it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb89089-468f-49c1-993f-752d7a9f7619",
   "metadata": {},
   "source": [
    "### THIS CONCLUDES THIS UNIT. PROCEED TO THE NEXT NOTEBOOK"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "serverless_spark": "{\"name\":\"projects/nikhim-iceberg-lab/locations/us-central1/sessions/iceberg-lab-16947\",\"uuid\":\"7b192709-4525-4de5-8984-02c3c6557b73\",\"createTime\":\"2023-02-10T14:05:51.785776Z\",\"jupyterSession\":{},\"spark\":{},\"runtimeInfo\":{\"endpoints\":{\"Spark History Server\":\"https://7suwyw62lfbjhcvtckfaxhuxcm-dot-us-central1.dataproc.googleusercontent.com/sparkhistory/?eventLogDirFilter=7b192709-4525-4de5-8984-02c3c6557b73\"}},\"state\":\"ACTIVE\",\"stateTime\":\"2023-02-10T14:06:44.043437Z\",\"creator\":\"nikhim@google.com\",\"runtimeConfig\":{\"version\":\"1.0.29\",\"properties\":{\"spark:spark.jars.packages\":\"org.apache.iceberg:iceberg-spark-runtime-3.2_2.12:1.1.0\",\"spark:spark.sql.catalog.hdp_ctlg\":\"org.apache.iceberg.spark.SparkCatalog\",\"spark:spark.sql.catalog.hdp_ctlg.type\":\"hadoop\",\"spark:spark.sql.catalog.hdp_ctlg.warehouse\":\"gs://iceberg-data-bucket-928505941962/iceberg-warehouse-dir\",\"spark:spark.sql.catalog.spark_catalog\":\"org.apache.iceberg.spark.SparkSessionCatalog\",\"spark:spark.sql.catalog.spark_catalog.type\":\"hive\",\"spark:spark.sql.extensions\":\"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\"spark:spark.executor.instances\":\"2\",\"spark:spark.driver.cores\":\"4\",\"spark:spark.executor.cores\":\"4\",\"spark:spark.dynamicAllocation.executorAllocationRatio\":\"0.3\",\"spark:spark.eventLog.dir\":\"gs://iceberg-sphs-bucket-928505941962/7b192709-4525-4de5-8984-02c3c6557b73/spark-job-history\"}},\"environmentConfig\":{\"executionConfig\":{\"serviceAccount\":\"iceberg-lab-sa@nikhim-iceberg-lab.iam.gserviceaccount.com\",\"subnetworkUri\":\"spark-subnet\",\"idleTtl\":\"14400s\"},\"peripheralsConfig\":{\"metastoreService\":\"projects/nikhim-iceberg-lab/locations/us-central1/services/iceberg-hms-928505941962\",\"sparkHistoryServerConfig\":{\"dataprocCluster\":\"projects/nikhim-iceberg-lab/regions/us-central1/clusters/iceberg-sphs-928505941962\"}}},\"stateHistory\":[{\"state\":\"CREATING\",\"stateStartTime\":\"2023-02-10T14:05:51.785776Z\"}]}",
  "serverless_spark_kernel_name": "remote-181e9fde776b586aa38a5ebc-pyspark",
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
